{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-FID Score Presentation\n",
    "## Necessary packages and functions call\n",
    "\n",
    "- Context-FID score: A useful metric measures how well the the synthetic time series windows ”fit” into the local context of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:05:16.594392Z",
     "start_time": "2024-11-11T16:05:13.793428Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.path.dirname('__file__'), './'))\n",
    "from Utils.context_fid import Context_FID\n",
    "from Utils.metric_utils import display_scores\n",
    "from Utils.cross_correlation import CrossCorrelLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  ETTh Morning Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ori shape is:  (20167, 24, 1)\n",
      "fake shape is:  (20167, 24, 1)\n",
      "ori shape is:  (20167, 24, 1)\n",
      "fake shape is:  (20167, 24, 1)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m context_fid_score \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[0;32m---> 28\u001b[0m     context_fid \u001b[38;5;241m=\u001b[39m \u001b[43mContext_FID\u001b[49m\u001b[43m(\u001b[49m\u001b[43mori_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mori_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     context_fid_score\u001b[38;5;241m.\u001b[39mappend(context_fid)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext-fid =\u001b[39m\u001b[38;5;124m'\u001b[39m, context_fid, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/duyin/test/timegantf/output/Utils/context_fid.py:25\u001b[0m, in \u001b[0;36mContext_FID\u001b[0;34m(ori_data, generated_data)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mContext_FID\u001b[39m(ori_data, generated_data):\n\u001b[1;32m     23\u001b[0m     model \u001b[38;5;241m=\u001b[39m TS2Vec(input_dims\u001b[38;5;241m=\u001b[39mori_data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, output_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m320\u001b[39m,\n\u001b[1;32m     24\u001b[0m                    max_train_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mori_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     ori_represenation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(ori_data, encoding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_series\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m     gen_represenation \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(generated_data, encoding_window\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_series\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/scratch/duyin/test/timegantf/output/Models/ts2vec/ts2vec.py:130\u001b[0m, in \u001b[0;36mTS2Vec.fit\u001b[0;34m(self, train_data, n_epochs, n_iters, verbose)\u001b[0m\n\u001b[1;32m    127\u001b[0m out2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_net(take_per_row(x, crop_offset \u001b[38;5;241m+\u001b[39m crop_left, crop_eright \u001b[38;5;241m-\u001b[39m crop_left))\n\u001b[1;32m    128\u001b[0m out2 \u001b[38;5;241m=\u001b[39m out2[:, :crop_l]\n\u001b[0;32m--> 130\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mhierarchical_contrastive_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemporal_unit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtemporal_unit\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    137\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/scratch/duyin/test/timegantf/output/Models/ts2vec/models/losses.py:14\u001b[0m, in \u001b[0;36mhierarchical_contrastive_loss\u001b[0;34m(z1, z2, alpha, temporal_unit)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m temporal_unit:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m         loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m alpha) \u001b[38;5;241m*\u001b[39m \u001b[43mtemporal_contrastive_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     16\u001b[0m z1 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(z1\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/scratch/duyin/test/timegantf/output/Models/ts2vec/models/losses.py:44\u001b[0m, in \u001b[0;36mtemporal_contrastive_loss\u001b[0;34m(z1, z2)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m z1\u001b[38;5;241m.\u001b[39mnew_tensor(\u001b[38;5;241m0.\u001b[39m)\n\u001b[1;32m     43\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([z1, z2], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# B x 2T x C\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# B x 2T x 2T\u001b[39;00m\n\u001b[1;32m     45\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtril(sim, diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, :, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]    \u001b[38;5;66;03m# B x 2T x (2T-1)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m logits \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtriu(sim, diagonal\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, :, \u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./etth_mp/ori_etth_mp24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./etth_mp/generate_etth_mp24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  etth Evening Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./etth_ep/ori_etth_ep24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./etth_ep/generate_etth_ep24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  Energy Morning Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./energy_mp/ori_energy_mp24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./energy_mp/generate_energy_mp24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  energy Evening Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./energy_ep/ori_energy_ep24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./energy_ep/generate_energy_ep24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  PEMS04 Morning Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T16:05:16.633192Z",
     "start_time": "2024-11-11T16:05:16.597089Z"
    }
   },
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./pems04_mp/ori_pems04_mp24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./pems04_mp/generate_pems04_mp24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  PEMS04 Evening Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./pems04_ep/ori_pems04_ep24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./pems04_ep/generate_pems04_ep24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  PEMS08 Morning Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./pems08_mp/ori_pems08_mp24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./pems08_mp/generate_pems08_mp24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading  PEMS08 Evening Peak\n",
    "\n",
    "Load original dataset and preprocess the loaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterations = 5\n",
    "iterations = 5\n",
    "\n",
    "ori_data = np.load('./pems08_ep/ori_pems08_ep24.npy')  # Uncomment the line if dataset other than Sine is used.\n",
    "fake_data = np.load('./pems08_ep/generate_pems08_ep24.npy')\n",
    "\n",
    "print('ori shape is: ', ori_data.shape)\n",
    "print('fake shape is: ', fake_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "#  CFS\n",
    "\n",
    "for j in range(2):\n",
    "\n",
    "    context_fid_score = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        context_fid = Context_FID(ori_data[:], fake_data[:ori_data.shape[0]])\n",
    "        context_fid_score.append(context_fid)\n",
    "        print(f'Iter {i}: ', 'context-fid =', context_fid, '\\n')\n",
    "\n",
    "    display_scores(context_fid_score)\n",
    "\n",
    "# Seed 12345 :  mp  10mile  0.292   0.309\n",
    "# seed 12345 :  ep  10mile  0.151\n",
    "\n",
    "\n",
    "# ett mp 1000 mile     0.1427\n",
    "# 看看 500 mile的     等训练完了试试看？    0.1567\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch201",
   "language": "python",
   "name": "torch201"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
